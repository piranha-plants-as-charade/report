\subsection{Melody Extraction}
\label{sec:melody_extraction}

The first step in Piranha Plants as Charade is to extract the melody from the input audio. The goal of this component is to take an audio file and output a sequence of notes, each defined by a midi pitch, start time, and duration. This problem can be broken down into two subproblems: pitch detection and note segmentation.

As a pre-processing step, we first apply Harmonic-Percussive Source Separation (HPSS) \autocite{HPSS:2010,HPSS:2014} to separate the harmonic and percussive components of the audio signal. We perform pitch detection on the harmonic component and use the percussive component to identify articulations in the melody, referred to as note onsets. Finally, we leverage both components to produce the final sequence of notes.

\subsubsection{Pitch Detection}

Pitch detection determines the fundamental frequency of a sound; we aim to produce a sequence of MIDI pitches representing the melody.

We explored a few different approaches, starting with cepstrum analysis and autocorrelation. However, these methods failed to handle the complexity of real audio recordings that are substantially noisier than synthetic data. We decided to use the PYIN algorithm as implemented in the LibROSA library, which is a state-of-the-art pitch detection algorithm that builds upon autocorrelation to handle audio more robustly.

%% CEPSTRUM

A tone consists of a fundamental frequency $f_0$ and harmonics $f_k = k \cdot f_0$ for $k \geq 1$, where $f_0 = f_1$. In the Fourier domain of the signal, this corresponds to a peak at the fundamental frequency and its harmonics. This forms a series of peaks at regular intervals. The idea of cepstrum is to identify this periodic pattern in the Fourier spectrum by applying the Fourier transform again.

Formally, the cepstrum $C$ is defined on a signal $y$ as follows:
$$C = \text{FFT}\left(\log\left(\left|\text{FFT}\left(y\right)\right|\right)\right)$$

The fundamental frequency of the signal can be found by identifying the first non-zero peak in the cepstrum:
$$f_0 = \frac{1}{q} = \frac{1}{\left(\arg\max_{k} C[k]\right)}$$
where $q$ is a quefrency, defined as the reciprocal of frequency.

However, this method is not robust to noise and did not perform reliably in our experiments \footnote{TODO: Link notebook here} using real audio recordings.

%% AUTOCORRELATION

Next, we explored autocorrelation as a method for pitch detection. The idea is to find the periodicity of the signal by comparing it to a delayed version of itself. The non-zero shift with the highest correlation between signals corresponds to the fundamental frequency of the signal. One formulation of the autocorrelation function, adapted from the Pearson correlation coefficient, is given by:
\begin{align*}
    r(k)
    &= \frac{\text{Cov}(y_n, y_{n-k})}{\text{Var}(y_n)} \\
    &= \frac{\sum_{n=k+1}^N (y_n - \mu_{y_n}) (y_{n-k} - \mu_{y_{n-k}})}{\sum_n (y_n - \mu_{y_n})^2}
\end{align*}

While this method yielded better results, it still showed some limitations in handling real audio recordings. Results were very sensitive to the choice of hyperparameters, such as the window size and correlation thresholds. Nevertheless, this served as an interesting exploration into a method that is the foundation for PYIN, the algorithm we decided to use.

%% PYIN

We ultimately decided to use the PYIN algorithm \autocite{PYIN:2014} due to both its robustness with real audio and its readily available implementation in the LibROSA library \footnote{\href{https://librosa.org/doc/0.11.0/generated/librosa.pyin.html}{librosa.org/doc/0.11.0/generated/librosa.pyin.html}}, which allowed us to focus on the higher-level aspects of our project. PYIN is a state-of-the-art pitch detection algorithm that builds upon autocorrelation to handle audio more robustly. At a high level, PYIN applies autocorrelation to detect pitch candidates and refines the result using a probabilistic thresholding model to filter out spurious frequencies.

After applying PYIN to the harmonic component of the audio signal, we have the estimated pitch in Hertz across time. We then apply multiple pitch shifts by a few cents \footnote{A logarithmic unit representing a hundredth of a semitone.} to find the best match, determined by the lowest error after rounding to the nearest MIDI semitone.

\subsubsection{Note Segmentation}

Once we have the pitch sequence, it must be segmented into individual notes. To do this, we count quantized time units from the start of the first identified pitch at the assumed tempo of the song. Within each interval, we take the mode of the pitches as the pitch at that time.

To determine breaks between notes, we end the previous note and begin a new one if either of the following conditions is met:

\begin{enumerate}
    \item The pitch changes from the previous interval.
    \item An onset is detected in the percussive component of the audio.
\end{enumerate}

Thus, we obtain a sequence of notes defining the melody to pass to the next component.
